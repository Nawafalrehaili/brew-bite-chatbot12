import streamlit as st
import pandas as pd
from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch

@st.cache_resource
def load_model():
    tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-small")
    model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-small")
    return tokenizer, model

@st.cache_data
def load_data():
    df = pd.read_csv("data.csv")
    paragraphs = []
    for _, row in df.iterrows():
        paragraph = f"ÙØ±Ø¹ {row['Ø§Ø³Ù… Ø§Ù„ÙØ±Ø¹']} ÙÙŠ {row['Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©']}, Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø¯ÙˆØ§Ù…: {row['Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø¯ÙˆØ§Ù…']}, Ø±Ø¶Ø§ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡: {row['Ø±Ø¶Ø§ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡']}, Ø±Ù‚Ù… Ø§Ù„Ù…Ø¯ÙŠØ±: {row['Ø±Ù‚Ù… Ø§Ù„Ù…Ø¯ÙŠØ±']}."
        paragraphs.append(paragraph)
    return "\n".join(paragraphs)

tokenizer, model = load_model()
context = load_data()

def generate_answer(question, context):
    input_text = f"Ø§Ù„Ø³Ø¤Ø§Ù„: {question}\nØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª: {context}\nØ§Ù„Ø¬ÙˆØ§Ø¨:"
    input_ids = tokenizer(input_text, return_tensors="pt", truncation=True).input_ids
    outputs = model.generate(input_ids, max_length=150)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

st.title("ğŸ¤– Ø´Ø§Øª Ø¨ÙˆØª ÙØ±ÙˆØ¹ Brew & Bite")

user_question = st.text_input("âœï¸ Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ:")

if user_question:
    answer = generate_answer(user_question, context)
    st.success(f"âœ… Ø§Ù„Ø±Ø¯: {answer}")
