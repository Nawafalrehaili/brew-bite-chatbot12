import streamlit as st
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

# Load model and tokenizer
@st.cache_resource
def load_model():
    model_name = "google/flan-t5-base"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    return tokenizer, model

tokenizer, model = load_model()

# Load data from CSV
@st.cache_data
def load_context_from_csv():
    df = pd.read_csv("data.csv")
    paragraphs = []
    for _, row in df.iterrows():
        paragraph = f"ÙØ±Ø¹ {row['Ø§Ø³Ù… Ø§Ù„ÙØ±Ø¹']} ÙŠÙ‚Ø¹ ÙÙŠ {row['Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©']}. Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø¹Ù…Ù„: {row['Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø¯ÙˆØ§Ù…']}. Ø±Ø¶Ø§ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡: {row['Ø±Ø¶Ø§ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡']}. Ø±Ù‚Ù… Ø§Ù„Ù…Ø¯ÙŠØ±: {row['Ø±Ù‚Ù… Ø§Ù„Ù…Ø¯ÙŠØ±']}."
        paragraphs.append(paragraph)
    return "\n".join(paragraphs)

context = load_context_from_csv()

# Title
st.title("ğŸ¤– Ø´Ø§Øª Ø¨ÙˆØª ÙØ±ÙˆØ¹ Brew & Bite")

# Input
user_question = st.text_input("ğŸ“ Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ:")

# Function to generate answer
def generate_answer(question, context):
    prompt = f"Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø©: {context}\nØ§Ù„Ø³Ø¤Ø§Ù„: {question}"
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True)
    outputs = model.generate(**inputs, max_length=100)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Output
if user_question:
    answer = generate_answer(user_question, context)
    st.success("ğŸ¤– Ø§Ù„Ø±Ø¯: " + answer)
